---
title: "Project"
author: "gfmgn101"
date: "December 7, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Problem statement
Develop a model that predicts whether or not an exercise is being done correctly. Use a data set from a study called the Qualitative Activity Recognition of Weight Lifting Exercises to train and predict the form of the exercise for 20 new observations.

##Approach
The problem statement requires a model that predict whether someone doing an exercise correctly and if incorrectly what is the incorrect form. This problem does not require a model with high interpretability, because what is of interest is the predictive aspect rather than understanding any one particular predictor. As such, a random forest model will be used due to their low bias.

##Libraries

```{r libraries}
library(readr)
library(caret)
```

##Loading
```{r loading}
filenameTrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
filenameTest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- read_csv(filenameTrain, na = c("","#DIV/0!", "NA")) ##DIV/0! comes in as character
testing <- read_csv(filenameTest)
```

##Cleaning
```{r cleaning}

length(training$X1)
training <- training[,colSums(is.na(training)) < nrow(training)] #for each col, sum the number of NAs. If the sum is less than the total number of rows then keep the column. creates a logical vector same length as number of columns. If the NAs equal the number or rows (entire row of NA) then exclude the column.
dim(training)

```

There are 19622 observations. Removing any columns that are completely NAs reduces the total columns to 154. However there are still NAs in the rows. Trying complete cases brings us the following: 

```{r completeCases}

complete <-complete.cases(training) #gives logical vector of complete cases
completeTraining <- training[complete,] #filters for those with complete cases
dim(completeTraining) #217 complete observations for 157 variables. Will there be enough overservations if we want to have a validation data set?
wideTrain <- completeTraining[c(-1,-3,-4,-5,-6)] #get rid of X1, timestamps (assuming when exercises recorded are irrelevant) and new window which are all 'yes'
dim(wideTrain)
```

Because there are number of columns that are mostly NAs, complete cases reduces a huge number of rows. going from 19622 to 217, with 154 columns. Preference would be to have more observations proportionally to the number of columns. What if we ignore those columns that are mostly NAs? Ignore columns that have more than 19000 NAs. 

```{r tallTrain}
omitNaColTrain <- training[,colSums(is.na(training))<19000]
dim(omitNaColTrain) #now left with 60 variables but a fuller set of observations
compNaColTrain <- omitNaColTrain[complete.cases(omitNaColTrain),]
dim(compNaColTrain) #complete cases took out one row to make it 19621 rows instead of 19622
tallTrain <- compNaColTrain[c(-1,-3,-4,-5,-6)] #take out X1, dates and new window
dim(tallTrain)
```

The resulting tallTrain dataframe is 19621 rows and 55 columns. 

##Random forest
As mentioned above, random forest is being used as a model as it has a high predictive capability though at the cost of interpretability, which is not as important in this use case. The Caret library is used to train the data on 'classe' and then the predict function is used to predict the classes using the model. Cross validation is not needed 


```{r randomForest, cache =TRUE}
#Random forest. Cross validation is already baked in as not all observations are used to create the decision trees.

tallTrainRF <- train(classe ~ ., data=tallTrain, method = 'rf')
tallTrainRF
dim(testing)
testdf <- testing[,colnames(tallTrain[,-55])] #testing dataframe has all the original columns, need to subset just to the ones that were used in the training set
predRF <- predict(tallTrainRF, testdf)
predRF
```
PredRF produces predictions for each of the 20 new observations.

##Determining which variables are the most important
```{r varImportance}
RFimp <- varImp(tallTrainRF)
plot(RFimp, top = 20)
```

##Conclusion
The twenty test cases follow the classes: B A B A A E D B A A B C B A E E A B B B
The num_window, roll belt, pitch forearm, yaw belt, and magnet dumbbell z are the five top contributing variales to the model. The first suggesting that the test cases were taken from the same window of time as similar exercises in the training test set.
